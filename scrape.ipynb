{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "max_pages = 10\n",
    "data = [('how do you get to Chad\\'s Gap from the bottom of Alta?', ['I heard ski patrol blew it up', 'my ankles are broken'])]\n",
    "ns_qa = pd.DataFrame(data, columns=['question', 'answers'])\n",
    "new_thread_data = [('what happened to Ian Compton?', ['he\\'s in TC heaven', 'he moved back east'])]\n",
    "new_thread = pd.DataFrame(new_thread_data, columns=['question', 'answers'])\n",
    "ns_qa = pd.concat([ns_qa, new_thread], ignore_index=True)\n",
    "\n",
    "def scrape_thread(page_url, ns_qa):\n",
    "    question = ''\n",
    "    answers = []\n",
    "\n",
    "    # scrape each thread page\n",
    "    try:\n",
    "        for i in range(1, max_pages+1):\n",
    "            response = requests.get(page_url)\n",
    "\n",
    "            # collect posts\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content with BeautifulSoup\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                posts = soup.find_all('div', class_='post-body')\n",
    "                # grab the op\n",
    "                if i == 1:\n",
    "                    question = soup.find('title').get_text().split(' -')[0] + ': ' +  posts[0].get_text(separator='\\n', strip=True).split('\\n')[-1]\n",
    "\n",
    "                for post in posts[1:]:\n",
    "                    lines = post.get_text(separator='\\n', strip=True).split('\\n')\n",
    "                    # if someone's quoting another user we only want the response\n",
    "                    answers.append(lines[-1])\n",
    "\n",
    "                # check for next page if its there go to the link\n",
    "                next_page_regex = rf'http.*page={i+1}\"'\n",
    "                matches = re.findall(next_page_regex, response.text)\n",
    "\n",
    "                if (matches):\n",
    "                    page_url = matches[0][:-1]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # save thread contents\n",
    "        new_thread_data = (question, answers)\n",
    "        new_thread = pd.DataFrame(data=[new_thread_data], columns=['question', 'answers'])\n",
    "        ns_qa = pd.concat([ns_qa, new_thread], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return ns_qa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NS dataset, 101 pages, 4001 threads saved.\n",
      "NS dataset, 102 pages, 4041 threads saved.\n",
      "NS dataset, 103 pages, 4081 threads saved.\n",
      "NS dataset, 104 pages, 4121 threads saved.\n",
      "NS dataset, 105 pages, 4161 threads saved.\n",
      "NS dataset, 106 pages, 4201 threads saved.\n",
      "NS dataset, 107 pages, 4241 threads saved.\n",
      "NS dataset, 108 pages, 4281 threads saved.\n",
      "NS dataset, 109 pages, 4321 threads saved.\n",
      "NS dataset, 110 pages, 4361 threads saved.\n",
      "NS dataset, 111 pages, 4401 threads saved.\n",
      "NS dataset, 112 pages, 4441 threads saved.\n",
      "NS dataset, 113 pages, 4481 threads saved.\n",
      "NS dataset, 114 pages, 4521 threads saved.\n",
      "NS dataset, 115 pages, 4561 threads saved.\n",
      "NS dataset, 116 pages, 4601 threads saved.\n",
      "NS dataset, 117 pages, 4641 threads saved.\n",
      "NS dataset, 118 pages, 4681 threads saved.\n",
      "NS dataset, 119 pages, 4721 threads saved.\n",
      "NS dataset, 120 pages, 4761 threads saved.\n",
      "NS dataset, 121 pages, 4801 threads saved.\n",
      "NS dataset, 122 pages, 4841 threads saved.\n",
      "NS dataset, 123 pages, 4881 threads saved.\n",
      "NS dataset, 124 pages, 4921 threads saved.\n",
      "NS dataset, 125 pages, 4961 threads saved.\n",
      "NS dataset, 126 pages, 5001 threads saved.\n",
      "NS dataset, 127 pages, 5041 threads saved.\n",
      "NS dataset, 128 pages, 5081 threads saved.\n",
      "NS dataset, 129 pages, 5121 threads saved.\n",
      "NS dataset, 130 pages, 5161 threads saved.\n",
      "NS dataset, 131 pages, 5201 threads saved.\n",
      "NS dataset, 132 pages, 5241 threads saved.\n",
      "NS dataset, 133 pages, 5281 threads saved.\n",
      "NS dataset, 134 pages, 5321 threads saved.\n",
      "NS dataset, 135 pages, 5361 threads saved.\n",
      "NS dataset, 136 pages, 5401 threads saved.\n",
      "NS dataset, 137 pages, 5441 threads saved.\n",
      "NS dataset, 138 pages, 5481 threads saved.\n",
      "NS dataset, 139 pages, 5521 threads saved.\n",
      "NS dataset, 140 pages, 5561 threads saved.\n",
      "NS dataset, 141 pages, 5601 threads saved.\n",
      "NS dataset, 142 pages, 5641 threads saved.\n",
      "NS dataset, 143 pages, 5681 threads saved.\n",
      "NS dataset, 144 pages, 5721 threads saved.\n",
      "NS dataset, 145 pages, 5761 threads saved.\n",
      "NS dataset, 146 pages, 5801 threads saved.\n",
      "NS dataset, 147 pages, 5841 threads saved.\n",
      "NS dataset, 148 pages, 5881 threads saved.\n",
      "NS dataset, 149 pages, 5921 threads saved.\n",
      "NS dataset, 150 pages, 5961 threads saved.\n",
      "NS dataset, 151 pages, 6001 threads saved.\n",
      "NS dataset, 152 pages, 6041 threads saved.\n",
      "NS dataset, 153 pages, 6081 threads saved.\n",
      "NS dataset, 154 pages, 6121 threads saved.\n",
      "NS dataset, 155 pages, 6161 threads saved.\n",
      "NS dataset, 156 pages, 6201 threads saved.\n",
      "NS dataset, 157 pages, 6241 threads saved.\n",
      "NS dataset, 158 pages, 6281 threads saved.\n",
      "NS dataset, 159 pages, 6321 threads saved.\n",
      "NS dataset, 160 pages, 6361 threads saved.\n",
      "NS dataset, 161 pages, 6401 threads saved.\n",
      "NS dataset, 162 pages, 6441 threads saved.\n",
      "NS dataset, 163 pages, 6481 threads saved.\n",
      "NS dataset, 164 pages, 6521 threads saved.\n",
      "NS dataset, 165 pages, 6561 threads saved.\n",
      "NS dataset, 166 pages, 6601 threads saved.\n",
      "NS dataset, 167 pages, 6641 threads saved.\n",
      "NS dataset, 168 pages, 6681 threads saved.\n",
      "NS dataset, 169 pages, 6721 threads saved.\n",
      "NS dataset, 170 pages, 6761 threads saved.\n",
      "NS dataset, 171 pages, 6801 threads saved.\n",
      "NS dataset, 172 pages, 6841 threads saved.\n",
      "NS dataset, 173 pages, 6881 threads saved.\n",
      "NS dataset, 174 pages, 6921 threads saved.\n",
      "NS dataset, 175 pages, 6961 threads saved.\n",
      "NS dataset, 176 pages, 7001 threads saved.\n",
      "NS dataset, 177 pages, 7041 threads saved.\n",
      "NS dataset, 178 pages, 7081 threads saved.\n",
      "NS dataset, 179 pages, 7121 threads saved.\n",
      "NS dataset, 180 pages, 7161 threads saved.\n",
      "NS dataset, 181 pages, 7201 threads saved.\n",
      "NS dataset, 182 pages, 7241 threads saved.\n",
      "NS dataset, 183 pages, 7281 threads saved.\n",
      "NS dataset, 184 pages, 7321 threads saved.\n",
      "NS dataset, 185 pages, 7361 threads saved.\n",
      "NS dataset, 186 pages, 7401 threads saved.\n",
      "NS dataset, 187 pages, 7441 threads saved.\n",
      "NS dataset, 188 pages, 7481 threads saved.\n",
      "NS dataset, 189 pages, 7521 threads saved.\n",
      "NS dataset, 190 pages, 7561 threads saved.\n",
      "NS dataset, 191 pages, 7601 threads saved.\n",
      "NS dataset, 192 pages, 7641 threads saved.\n",
      "NS dataset, 193 pages, 7681 threads saved.\n",
      "NS dataset, 194 pages, 7721 threads saved.\n",
      "NS dataset, 195 pages, 7761 threads saved.\n",
      "NS dataset, 196 pages, 7801 threads saved.\n",
      "NS dataset, 197 pages, 7841 threads saved.\n",
      "NS dataset, 198 pages, 7881 threads saved.\n",
      "NS dataset, 199 pages, 7921 threads saved.\n"
     ]
    }
   ],
   "source": [
    "for i in range(101, 200):\n",
    "    forum_page = f\"https://www.newschoolers.com/forum/1/Ski-Gabber?page={i}\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(forum_page)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all thread titles and their links\n",
    "    thread_elements = soup.find_all(\"a\", class_=\"thread\")\n",
    "\n",
    "    # Extract thread titles and links and store them in a list of tuples\n",
    "    thread_list = [(thread.h2.get_text(strip=True), thread[\"href\"]) for thread in thread_elements]\n",
    "\n",
    "    # Print the list of tuples (thread title, thread link)\n",
    "    for index, thread in enumerate(thread_list):\n",
    "        ns_qa = scrape_thread(thread[1], ns_qa)\n",
    "        # print(f'thread {index}, {thread[0]} scraped.')\n",
    "    \n",
    "    ns_qa['question'] = ns_qa['question'].str.replace('|', '')\n",
    "    ns_qa['answers'] = ns_qa['answers'].apply(lambda x: [item.replace('|', '') for item in x])\n",
    "    ns_qa.to_csv('ns_qa_p101_200.csv', index=False, mode='w')\n",
    "    print(f'NS dataset, {i} pages, {len(ns_qa)} threads saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = pd.read_csv('ns_qa_p1_200.csv')\n",
    "\n",
    "def clean_strings(answer_list):\n",
    "    return [sentence.replace(\"\\\\\", \"\") for sentence in ast.literal_eval(answer_list)]\n",
    "\n",
    "dset['answers'] = dset['answers'].apply(clean_strings)\n",
    "\n",
    "print(dset['question'][0], dset['answers'][0])\n",
    "\n",
    "dset['pretoken_input'] = None\n",
    "\n",
    "for j in range(len(dset['question'])):\n",
    "    message = [{\"role\": \"user\", \"content\": dset['question'][j]}]\n",
    "    for i, resp in enumerate(dset['answers'][j]):\n",
    "        # alternate between \"user\" and \"assistant\" -> \"op\" and \"other\"\n",
    "        if i % 2 == 0:\n",
    "            message.append({\"role\": \"assistant\", \"content\": resp})\n",
    "        else:\n",
    "            message.append({\"role\": \"user\", \"content\": resp})\n",
    "            \n",
    "    dset.at[j, 'pretoken_input'] = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dset = Dataset.from_pandas(dset)\n",
    "hf_dset = hf_dset.add_column('formatted_pretoken_input', [tokenizer.apply_chat_template(msg, tokenize=False) for msg in hf_dset['pretoken_input']])\n",
    "\n",
    "hf_dset['formatted_pretoken_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dset = Dataset.from_pandas(dset)\n",
    "hf_dset = hf_dset.add_column('formatted_pretoken_input', [tokenizer.apply_chat_template(msg, tokenize=False) for msg in hf_dset['pretoken_input']])\n",
    "# hf_dset = hf_dset.add_column('tokenized_input', [tokenizer.encode(msg, padding='max_length', max_length=512, truncation=True) for msg in hf_dset['formatted_pretoken_input']]) # SFTTrainer won't let me token inputs myself\n",
    "hf_dset = hf_dset.select_columns('formatted_pretoken_input')\n",
    "hf_dset = hf_dset.train_test_split(test_size=0.1)\n",
    "# hf_dset.save_to_disk('./hf_dset') # commented to prevent overwrite\n",
    "hf_dset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92003710e25812d27cc3758aea97d0feddf87267d786b8119bda72abde84b302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
