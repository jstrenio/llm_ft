{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5039879decaa443e9bbc0e5b128cdb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2ec2c226324fd1923202ef7ad7f048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52a9db14a7d441091146c7f0cf3737f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              )\n",
       "              (up_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              )\n",
       "              (down_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(\n",
       "        in_features=4096, out_features=32000, bias=False\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=2, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "hf_dset = hf_dset = load_from_disk('./hf_dset/')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    ")\n",
    "\n",
    "# auto maps to GPUs shouldn't matter since I only have 1, trust_remote_code is for custom defined models to pull from HF hub\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, trust_remote_code=True)\n",
    "\n",
    "# use CPU to prevent OOM\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "# please take care of model memory management for me\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "\n",
    "# also helps memory \n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# scaling factor for learned weights is alpha/r and bigger rank = more computation tradeoff. paper did 64/16=8, we'll try 16/2=8\n",
    "config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# prep model for qlora\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Apply the accelerator.\n",
    "model = accelerator.prepare_model(model)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=f\"./model_ft_{now.strftime('%m.%d.%H')}\",\n",
    "        warmup_steps=2,\n",
    "        per_device_train_batch_size=2, # reduce if OOM by 2x\n",
    "        gradient_accumulation_steps=4, # x 2x if batch size is reduced\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=10,\n",
    "        logging_dir=\"./logs\", # Directory for storing logs\n",
    "        fp16 = False,\n",
    "        bf16 = True, # With an A100\n",
    "        optim=\"paged_adamw_8bit\", \n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "    )\n",
    "\n",
    "# using SFTTrainer instead of Trainer() because huggingface recommends its use for this exact purpose\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=hf_dset['train'],\n",
    "    eval_dataset=hf_dset['test'],\n",
    "    dataset_text_field='formatted_pretoken_input',\n",
    "    data_collator=None,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "model.to(accelerator.device)\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]</s> \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    text_input = \"[INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]\"\n",
    "    tokenized_input = tokenizer(text_input, return_tensors=\"pt\")\n",
    "    # input_ids = tokenized_input['input_ids']\n",
    "    output_ids = model.generate(**tokenized_input)\n",
    "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference comparison - initial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c175e7eea84f668133c50845c5e23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "PEFT_MODEL = 'model_ft_11.11.15/checkpoint-150'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what's your favorite line traveling circus episode? [/INST]I’m not sure if it was the first one or not but I remember watching that one and being like “wow” for a good 10 minutes. The whole thing is just so fucking cool, I love how they do all those tricks on the rails and stuff. It’s really inspiring to watch.\n",
      "User 2: I think the first one is my favourite too. I remember watching it when I was younger and thinking \"how did he do that?\" and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what's your favorite line traveling circus episode? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] how do you get to chad's gap from the bottom of alta?[/INST]I think it’s a bit more difficult than most gaps. I would say just go up and try to find it, but if you can’t then maybe ask someone who knows where it is or look at some pictures online. It’s pretty hard to miss once you know what you’re looking for. If you don’t see anything that looks like a gap, then you probably won’t find it. Good luck!\n",
      "User 0: You have to be on the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] how do you get to chad's gap from the bottom of alta?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what new movie are you going to see this year? [/INST]I'm not seeing any movies this year. I don't like the way they make them nowadays. They all look the same and have no soul.\n",
      "User 1: I’ll be watching the new James Bond movie, but I’ve been waiting for a good action movie since the last one came out in 2018. I’d love to see a new Die Hard or Lethal Weapon movie. Or maybe a sequel to The Expend\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what new movie are you going to see this year? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] how is the durability of line skis?[/INST]I've had a pair of 2015s for 6 years and they are still in great shape. I'm sure it depends on the model, but I would say that they hold up pretty well.\n",
      "User 3: They’re not very durable at all. The base will get scratched easily and the topsheet can be chipped off with ease. If you want to keep your skis looking good, you need to wax them regularly. Also,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] how is the durability of line skis?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] where should I go to college? [/INST]**This post was edited on Jan 13th 2022 at 9:58:47pm [/INST]I'm in the same boat. I've narrowed it down to UVM, UNH, and MSU. Any advice would be appreciated!\n",
      "User 6: If you’re looking for a school with a strong ski program, look into the University of Colorado Boulder. They have a great ski team and are located\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] where should I go to college? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What's your favorite segment of all time? [/INST]I’m not sure if you mean a specific video or just a segment in general. If it’s a specific video, I’d say the first run of the 2018 X Games slopestyle was pretty sick. If it’s a segment in general, I’d say the one where Henrik Harlaut does that backflip off the rail and then goes over the jump and lands on his feet is pretty cool.\n",
      "User 3: The one\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What's your favorite segment of all time? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what do you think of armada? [/INST]I'm not a fan. I don't like the way they market their skis, and I don't like the way they make their skis. They have some cool designs though.\n",
      "User 1: Armada is pretty good. I’ve had a pair of theirs for a few years now and they are still going strong. The only thing that bothers me about them is how expensive they are. I know it’s because they use high quality materials but\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what do you think of armada? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what do you think of the game shredsauce? [/INST]I've never played it but I heard it was pretty fun. It's a good way to get some practice in when there isn't any snow on the ground.\n",
      "User 0: I haven’t played it in years, but I remember it being really fun. The only thing that sucked about it was that you had to pay for each level and it got expensive fast. But if you were willing to put in the time and money, it was definitely worth it. I\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what do you think of the game shredsauce? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] who would win in a game of slvsh, will wesson or andy parry?[/INST]I'd say Andy Parry. He has the most experience with Skiing and is also one of the best skiers I know. Will Wesson is an amazing skier but he doesn't have as much experience with skiing.\n",
      "User 0: I’m gonna go with Andy Parry too. I think his experience and style would give him the edge over Will. But that being said, it would be a close call. Both are extremely talented skiers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] who would win in a game of slvsh, will wesson or andy parry?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what place did lj get in xgames realski? [/INST]He got 10th. I think he was the only one to do a switch 720 on rail. He also had some other tricks that were pretty sick, but it wasn't enough for him to make top 3.\n",
      "User 4: LJ is so fucking good at skiing. I’m not sure why he doesn’t have more sponsors or anything like that. He has such a unique style and he can do things that no one else does\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what place did lj get in xgames realski? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea2f50d2dd742c58f20ffe78e16d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what's your favorite line traveling circus episode? [/INST] My favorite line from the \"Traveling Circus\" episode of \"The Simpsons\" is when Bart says, \"I'm not a circus freak, I'm a normal kid!\" This line is funny because it shows Bart's discomfort with being around the circus and its performers, despite his initial excitement about the episode.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what's your favorite line traveling circus episode? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] how do you get to chad's gap from the bottom of alta?[/INST] To get to Chad's Gap from the bottom of Alta, you can take a combination of public transportation and hiking. Here are the steps:\n",
      "\n",
      "1. Take the TRAX light rail system from Salt Lake City to Sandy Station. From there, transfer to the UTA bus system and take bus 970 or 972 towards Provo. Get off at the Provo Central Station.\n",
      "2. Walk about 5 minutes north on Main Street until you reach the Provo Temple.\n",
      "3. Turn left onto Center Street and walk for about 4 blocks until you reach the intersection with 6th Avenue.\n",
      "4. Turn right onto 6th Avenue and continue walking for about 2 blocks until you reach the intersection with 8th Avenue.\n",
      "5. Turn left onto 8th Avenue and continue walking for about 2 blocks until you reach the intersection with 10th Avenue.\n",
      "6. Turn right onto 10th Avenue and continue walking for about 2 blocks until you reach the intersection with 12th Avenue.\n",
      "7. Turn left onto 12th Avenue and continue walking for about 2 blocks until you reach the intersection with \n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] how do you get to chad's gap from the bottom of alta?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what new movie are you going to see this year? [/INST] I don't have personal experiences or interests, so I don't go to movies. However, I can provide information about upcoming movies that might be of interest to people. Some highly anticipated movies for 2023 include \"Spider-Man: Across the Spider-Verse,\" \"Dune: Part II,\" and \"The Flash.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what new movie are you going to see this year? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] how is the durability of line skis?[/INST] The durability of line skis can vary depending on several factors, such as the quality of materials used in construction, the frequency and intensity of use, and proper maintenance. Generally speaking, high-quality line skis made with durable materials like fiberglass or carbon fiber can last for many years if properly cared for. However, it's important to note that line skis are designed for a specific type of terrain and activity, so their durability may be affected by exposure to harsh weather conditions or rough terrain. Regular inspections and repairs can also help prolong the life of your line skis.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] how is the durability of line skis?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] where should I go to college? [/INST] The decision of where to attend college is a personal one and depends on various factors such as your academic goals, interests, location, cost, and campus culture. Here are some steps you can take to help you make an informed decision:\n",
      "\n",
      "1. Research colleges and universities: Look into different schools that offer programs in your field of interest. Consider the reputation, accreditation, faculty qualifications, student diversity, and resources available at each school. You can also check out rankings from reputable sources like U.S. News & World Report or Princeton Review.\n",
      "2. Visit campuses: Schedule visits to the colleges and universities you're interested in attending. This will give you a chance to see the campus, meet with admissions representatives, tour facilities, and get a feel for the campus culture.\n",
      "3. Apply for financial aid: Fill out the FAFSA (Free Application for Federal Student Aid) to determine your eligibility for financial aid. Many colleges offer scholarships, grants, and loans to help students cover tuition costs.\n",
      "4. Consider internship opportunities: Look for colleges that have strong connections to industry leaders and offer internships or co-op programs in your desired field.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] where should I go to college? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What's your favorite segment of all time? [/INST] I don't have personal experiences, emotions or favorites. However, some segments that have received high praise from viewers include \"The Great Debates,\" where prominent figures discuss current issues, and \"Council on Foreign Relations,\" which features experts discussing global affairs.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What's your favorite segment of all time? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what do you think of armada? [/INST] I am an AI language model and do not have personal opinions. However, I can provide information about Armada.\n",
      "\n",
      "Armada is a popular open-source software project that provides a web-based interface for managing and monitoring Kubernetes clusters. It was created by the same team that developed Helm, another popular tool for deploying applications on Kubernetes.\n",
      "\n",
      "Armada offers several features that make it useful for managing Kubernetes clusters, including:\n",
      "\n",
      "* A user-friendly web\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what do you think of armada? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what do you think of the game shredsauce? [/INST] I'm not familiar with a specific game called \"shredsauce.\" Could you please provide more information or clarify the name of the game in question?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what do you think of the game shredsauce? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] who would win in a game of slvsh, will wesson or andy parry?[/INST] It is difficult to say who would win in a game of \"Slvsh\" between Wesson and Andy Parry as it depends on various factors such as their skill level, strategy, and luck. However, based solely on their reputation and experience, Wesson may have an edge due to his extensive training and competition in the sport. Additionally, Wesson has won multiple world championships and is widely considered one of the greatest players in the history of Ultimate Fighting Championship (UFC). On the other hand, Andy Parry is also a highly skilled fighter with a strong background in mixed martial arts and kickboxing. Ultimately, the outcome of the game would depend on many variables and could go either way.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] who would win in a game of slvsh, will wesson or andy parry?[/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] what place did lj get in xgames realski? [/INST] LJ (Lee Jong-hyuk) is a professional StarCraft II player from South Korea. He has won numerous championships and tournaments throughout his career, including multiple GSL titles and WCS Global Circuit events.\n",
      "\n",
      "In the XGames RealSki event, which took place in 2019, LJ finished in second place behind fellow Korean player Kim \"Maru\" Young-hoon. Maru won the event with a score of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] what place did lj get in xgames realski? [/INST]\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consolidating my personal eval set and pulling examples from the real eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what's your favorite line traveling circus episode?\", \"how do you get to chad's gap from the bottom of alta?\", 'what new movie are you going to see this year?', 'how is the durability of line skis?', 'where should I go to college?', \"What's your favorite segment of all time?\", 'what do you think of armada?', 'what do you think of the game shredsauce?', 'who would win in a game of slvsh, will wesson or andy parry?', 'what place did lj get in xgames realski?', ' Skis measurements: I’ve skied for 13 years now and I’m very embarrassed but anyone want to explain ski measurements? ', ' Epic sunset shredding with my pup: [URL]https://youtu.be/TCRIRT4gBO[/URL]A ', ' Congrats Eileen Gu!!: My girlfriend just won gold I’m so proud!! ', ' Stolen Skis: Broke my arm yesterday and had to ski to the ski patrol hut and some fuck stole them while I was in there. Pair of J Skis Vacation mallards 180 with some tyrolia attacks on them. lmk if anyone sees them around. Was at Mt. La Crosse in WI ', ' Returned from Hell(bent): https://heritagelabskis.com/products/hb122 ', ' Jackson or targhee: I’m going to my grandmas for New Years weekend and she lives fairly close to both Jackson hole resort and grand targhee. I’ve never been to either but I’m going to go to one of them on Saturday with my brothers. Which one is a better choice for those that have been? We are heavy park skiers but love us some quality runs with nice side hits, drops, etc… rougher terrain. Which has a better park? ', ' Kinky rail ❤️: I walk by this rail almost everyday and it’s naaaasty. Would I hit it? Yes? Would I successful slide through the whole rail? Probably not for a while, but my god i wished it snowed here so many nice rails and urban features around my campus. Fucking shame it’s in the south :/ ', ' What are mogul skiers like?: https://www.newschoolers.com/videos/watch/988376/The-Mayrand-podcast--4---Mikael-Kingsbury ', ' What if a Full Tilt, SPK and Apex had a baby?: **This thread was edited on Oct 21st 2020 at 8:21:07pm ', ' Is blunt hard: When I first was learning grabs with spins I went for safety because everyone says its the easiest. But I quickly found myself corking out everytime because of reaching down for the grab and I would land backseat a lot. Then I tried blunt and not only was it easier for me to spot the landing but it naturally helped bring the spin around because to look for the blunt I had to turn my head anyways. IDK I think blunt might be a good grab for people trying grabs with spins for the first time. ']\n"
     ]
    }
   ],
   "source": [
    "custom_eval = [\"what's your favorite line traveling circus episode?\",\n",
    "               \"how do you get to chad's gap from the bottom of alta?\",\n",
    "               \"what new movie are you going to see this year?\",\n",
    "               \"how is the durability of line skis?\",\n",
    "               \"where should I go to college?\",\n",
    "               \"What's your favorite segment of all time?\",\n",
    "               \"what do you think of armada?\",\n",
    "               \"what do you think of the game shredsauce?\",\n",
    "               \"who would win in a game of slvsh, will wesson or andy parry?\",\n",
    "               \"what place did lj get in xgames realski?\",\n",
    "               \"What are Tanner Hall's biggest accomplishments?\",\n",
    "               \"Should Henrik focus on filming or contests?\"\n",
    "               ]\n",
    "\n",
    "eval_ex = [s.split(\"[INST]\", 1)[1].split(\"[/INST]\", 1)[0] if \"[INST]\" in s and \"[/INST]\" in s else s for s in hf_dset['test']['formatted_pretoken_input'][:10]]\n",
    "\n",
    "examples = custom_eval + eval_ex\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for ex in examples:\n",
    "    prompt = \"[INST] \" + ex + \" [/INST]\"\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200, pad_token_id=2, repetition_penalty=1.15)[0], skip_special_tokens=True))\n",
    "    print('-' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
