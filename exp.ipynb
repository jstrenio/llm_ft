{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mistral-7b inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
        "\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n",
        "\"[INST] Do you have mayonnaise recipes? [/INST]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "model_inputs = encodeds.to(\"cuda\")\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=256, do_sample=False)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beb853f485944b1ca273a268a0e84887",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import subprocess\n",
        "import os\n",
        "os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\n",
        "import speech_recognition as sr\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
        "# disables training\n",
        "model.eval() \n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "r = sr.Recognizer()\n",
        "\n",
        "def stt():\n",
        "    text = ''\n",
        "    with sr.Microphone() as source:\n",
        "        clear_output(wait=True)\n",
        "        r.adjust_for_ambient_noise(source)\n",
        "        print(\"Speak something:\")\n",
        "        audio = r.listen(source)\n",
        "\n",
        "    try:\n",
        "        text = r.recognize_google(audio)\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(\"Error with the service: {0}\".format(e))\n",
        "    \n",
        "    return text\n",
        "\n",
        "def tts(text):\n",
        "    command = f'echo \"{text}\" | festival --tts 2>/dev/null'\n",
        "    subprocess.call(command, shell=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "model_inputs = encodeds.to(\"cuda\")\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=256, do_sample=False)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## speaking to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = []\n",
        "question = ''\n",
        "\n",
        "while (question != 'shut down'):\n",
        "    question = stt()\n",
        "    context.append({'role':'user','content':question})\n",
        "    print(f'lj: {question}')\n",
        "    if question == 'shut down':\n",
        "        break\n",
        "\n",
        "    # tokenize -> inference -> decode\n",
        "    model_input = tokenizer.apply_chat_template(context, return_tensors=\"pt\").to('cuda')\n",
        "    generated_ids = model.generate(model_input, max_new_tokens=256, do_sample=False)\n",
        "    raw_resp = tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    # if it followed the formatting, remove everything before the response, remove unrecognized characters\n",
        "    i_clip = raw_resp.rfind(\"[/INST]\")\n",
        "    if i_clip != -1:\n",
        "        raw_resp = raw_resp[i_clip + len(\"[/INST]\"):]\n",
        "    i_end = raw_resp.rfind(\"</s>\")\n",
        "    if i_end != -1:\n",
        "        raw_resp = raw_resp[:i_end]\n",
        "    resp = re.sub(r'[^\\x00-\\x7F]+', '', raw_resp)\n",
        "    context.append({'role':'assistant','content':resp})\n",
        "    print(f'misT: {resp}')\n",
        "    tts(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## typing to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = []\n",
        "question = ''\n",
        "\n",
        "while (question != 'shut down'):\n",
        "    question = input('question: ')\n",
        "    context.append({'role':'user','content':question})\n",
        "    print(f'lj: {question}')\n",
        "    if question == 'shut down':\n",
        "        break\n",
        "    # tokenize -> inference -> decode\n",
        "    model_input = tokenizer.apply_chat_template(context, return_tensors=\"pt\").to('cuda')\n",
        "    generated_ids = model.generate(model_input, max_new_tokens=256, do_sample=False)\n",
        "    raw_resp = tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    # if it followed the formatting, remove everything before the response, remove unrecognized characters\n",
        "    i_clip = raw_resp.rfind(\"[/INST]\")\n",
        "    if i_clip != -1:\n",
        "        raw_resp = raw_resp[i_clip + len(\"[/INST]\"):]\n",
        "    i_end = raw_resp.rfind(\"</s>\")\n",
        "    if i_end != -1:\n",
        "        raw_resp = raw_resp[:i_end]\n",
        "    resp = re.sub(r'[^\\x00-\\x7F]+', '', raw_resp)\n",
        "    context.append({'role':'assistant','content':resp})\n",
        "    print(f'misT: {resp}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    padding='max_length',\n",
        "    max_length=512,\n",
        "    truncation=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>  [INST] Do you have mayonnaise recipes? [/INST]\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "tokenizer.decode(encodeds[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how do you get to Chad's Gap from the bottom of Alta? ['I heard ski patrol blew it up', 'my ankles are broken']\n"
          ]
        }
      ],
      "source": [
        "dset = pd.read_csv('ns_qa_p1_200.csv')\n",
        "\n",
        "def clean_strings(answer_list):\n",
        "    return [sentence.replace(\"\\\\\", \"\") for sentence in ast.literal_eval(answer_list)]\n",
        "\n",
        "dset['answers'] = dset['answers'].apply(clean_strings)\n",
        "\n",
        "print(dset['question'][0], dset['answers'][0])\n",
        "\n",
        "dset['pretoken_input'] = None\n",
        "\n",
        "for j in range(len(dset['question'])):\n",
        "    message = [{\"role\": \"user\", \"content\": dset['question'][j]}]\n",
        "    for i, resp in enumerate(dset['answers'][j]):\n",
        "        # alternate between \"user\" and \"assistant\" -> \"op\" and \"other\"\n",
        "        if i % 2 == 0:\n",
        "            message.append({\"role\": \"assistant\", \"content\": resp})\n",
        "        else:\n",
        "            message.append({\"role\": \"user\", \"content\": resp})\n",
        "            \n",
        "    dset.at[j, 'pretoken_input'] = message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s>[INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]I heard ski patrol blew it up</s> [INST] my ankles are broken [/INST]\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_dset = Dataset.from_pandas(dset)\n",
        "hf_dset = hf_dset.add_column('formatted_pretoken_input', [tokenizer.apply_chat_template(msg, tokenize=False) for msg in hf_dset['pretoken_input']])\n",
        "\n",
        "hf_dset['formatted_pretoken_input'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e24a2b913204bb69928a4b7317f627f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7128 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa4edebfa0f6486389f78d69a845ac01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/793 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['formatted_pretoken_input'],\n",
              "        num_rows: 7128\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['formatted_pretoken_input'],\n",
              "        num_rows: 793\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_dset = Dataset.from_pandas(dset)\n",
        "hf_dset = hf_dset.add_column('formatted_pretoken_input', [tokenizer.apply_chat_template(msg, tokenize=False) for msg in hf_dset['pretoken_input']])\n",
        "# hf_dset = hf_dset.add_column('tokenized_input', [tokenizer.encode(msg, padding='max_length', max_length=512, truncation=True) for msg in hf_dset['formatted_pretoken_input']]) # SFTTrainer won't let me token inputs myself\n",
        "hf_dset = hf_dset.select_columns('formatted_pretoken_input')\n",
        "hf_dset = hf_dset.train_test_split(test_size=0.1)\n",
        "# hf_dset.save_to_disk('./hf_dset') # commented to prevent overwrite\n",
        "hf_dset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s>[INST] Update To Trying to Start a Rail Jam: So far i have gotten a confirmation from Honey Wax (@ridehoneywax) and support from ski addiction (ski_addiction). Also got advice from Mathew (@midwest_skier). As of now i’m just looking for one more sponsor before i propose the plan to my local hill. I have an outline started but still needs some work. thank you to all who’ve helped and please reach out if there is anymore advice or contacts of people i should reach out to. [/INST]Where is this at? I'd be interested in helping in the ways that I can. Getting sponsors is an important part.</s> [INST] this is at little switzerland slinger Wi. it is a tiny local hill but all our riders would love to see an event hosted. [/INST]**This post was edited on Nov 11th 2022 at 11:34:16am</s> [INST] Reach out to as many snowboard/skate shops as you can [/INST]Some local businesses typically sponsor the smaller events in my area, even if they aren’t snow sport related</s> [INST] **This post was edited on Nov 11th 2022 at 3:14:46pm [/INST]Shoot me a DM on here or IG and give me details and I'll see if I can help. Definitely can send t-shirts or something at the very least</s> [INST] Aye yo this looks like it might actually happen... so stocked for this [/INST]no way it's in fact bennie 07</s> [INST] Local shops are a good place to start.  When I used to run the PIST rail jam in Pittsburgh, we had a good relationship with a local shop.  They did a lot of the leg work finding brands that were willing to donate gear.  You'll have a much easier time dealing with the social media person at a local shop than the social media person from a big brand. [/INST]gilson once again supporting rail jams, love to see it</s> [INST] Good luck! Rail jams are mad fun [/INST]\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_dset['train']['formatted_pretoken_input'][2] # first converation, first message of conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s>[INST] Update To Trying to Start a Rail Jam: So far i have gotten a confirmation from Honey Wax (@ridehoneywax) and support from ski addiction (ski_addiction). Also got advice from Mathew (@midwest_skier). As of now i’m just looking for one more sponsor before i propose the plan to my local hill. I have an outline started but still needs some work. thank you to all who’ve helped and please reach out if there is anymore advice or contacts of people i should reach out to. [/INST]Where is this at? I'd be interested in helping in the ways that I can. Getting sponsors is an important part.</s> [INST] this is at little switzerland slinger Wi. it is a tiny local hill but all our riders would love to see an event hosted. [/INST]**This post was edited on Nov 11th 2022 at 11:34:16am</s> [INST] Reach out to as many snowboard/skate shops as you can [/INST]Some local businesses typically sponsor the smaller events in my area, even if they aren’t snow sport related</s> [INST] **This post was edited on Nov 11th 2022 at 3:14:46pm [/INST]Shoot me a DM on here or IG and give me details and I'll see if I can help. Definitely can send t-shirts or something at the very least</s> [INST] Aye yo this looks like it might actually happen... so stocked for this [/INST]no way it's in fact bennie 07</s> [INST] Local shops are a good place to start.  When I used to run the PIST rail jam in Pittsburgh, we had a good relationship with a local shop.  They did a lot of the leg work finding brands that were willing to donate gear.  You'll have a much easier time dealing with the social media person at a local shop than the social media person from a big brand. [/INST]gilson once again supporting rail jams, love to see it</s> [INST] Good luck! Rail jams are mad fun [/INST]\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "hf_dset = load_from_disk('./hf_dset/')\n",
        "hf_dset['train']['formatted_pretoken_input'][2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### model prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,            # load model in 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
        "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
        ")\n",
        "\n",
        "# auto maps to GPUs shouldn't matter since I only have 1, trust_remote_code is for custom defined models to pull from HF hub\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the number of available GPUs\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "# List the available GPUs\n",
        "gpu_list = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
        "\n",
        "# Print GPU memory information\n",
        "for i, gpu in enumerate(gpu_list):\n",
        "    device = torch.device(f\"cuda:{i}\")\n",
        "    mem_info = torch.cuda.get_device_properties(device)\n",
        "    print(f\"GPU {i} ({gpu}):\")\n",
        "    print(f\"  Total Memory: {mem_info.total_memory / (1024**3):.1f} GB\")\n",
        "    print(f\"  Free Memory: {torch.cuda.max_memory_allocated(i) / (1024**3):.1f} GB\")\n",
        "    print(f\"  Allocated Memory: {torch.cuda.memory_allocated(i) / (1024**3):.1f} GB\")\n",
        "    print(f\"  Cached Memory: {torch.cuda.memory_reserved(i) / (1024**3):.1f} GB\")\n",
        "    print(model.device)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 0 || all params: 3752071168 || trainable%: 0.0\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    \n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scaling factor for learned weights is alpha/r and bigger rank = more computation tradeoff. paper did 64/16=8, we'll try 16/2=8\n",
        "config = LoraConfig(\n",
        "    r=2,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
        "model = accelerator.prepare_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    print(\"More than one GPU\")\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./model_ft\",\n",
        "        warmup_steps=2,\n",
        "        per_device_train_batch_size=1, # reduce if OOM by 2x\n",
        "        gradient_accumulation_steps=1, # x 2x if batch size is reduced\n",
        "        max_steps=100,\n",
        "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
        "        logging_steps=10,\n",
        "        logging_dir=\"./logs\", # Directory for storing logs\n",
        "        fp16 = False,\n",
        "        bf16 = True, # With an A100\n",
        "        optim=\"paged_adamw_8bit\", \n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "    )\n",
        "\n",
        "# not using\n",
        "# max_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\n",
        "# report_to=\"wandb\",           # Weights & baises analytics\n",
        "# run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run\n",
        "# warmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\n",
        "# lr_scheduler_type = \"cosine\"  # learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=hf_dset['train'],\n",
        "    eval_dataset=hf_dset['test'],\n",
        "    dataset_text_field='formatted_pretoken_input',\n",
        "    data_collator=None,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "model.to(accelerator.device)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5334c0bf19c4cf9a672301ab849eb4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "base_model_id='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "PEFT_MODEL = './model_ft/checkpoint-100'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,            # load model in 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
        "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, trust_remote_code=True)\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
        "model = model.merge_and_unload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    padding='max_length',\n",
        "    max_length=512,\n",
        "    truncation=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4879821426454cb2bb9ef0d93309fd2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "who is tanner hall?\n",
            "User 1: Tanner Hall is a character in the TV show \"The Office\" played by actor Dwight Schrute. He is the Assistant Regional Manager of the Dunder Mifflin Paper Company branch in Scranton, Pennsylvania. He is known for his obsession with beets, his love of the show \"The Office\" and his desire to become the Regional Manager.\n",
            "\n",
            "Tanner Hall is portrayed as a quirky and eccentric character who is often misunderstood by his coworkers. He is also known for his love of beets and his desire to become the Regional Manager. He is often seen eating beets and talking about them, much to the amusement of his coworkers.\n",
            "\n",
            "Tanner Hall is a fan favorite and is often praised for his portrayal in the show. He is known for his unique personality and his love of beets.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "base_model_id='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "PEFT_MODEL = './model_ft/checkpoint-100'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, load_in_4bit=True)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=200,\n",
        ")\n",
        "\n",
        "prompt = \"<s>[INST] how do you get to chads gap from the bottom of alta? [/INST]\"\n",
        "\n",
        "print(pipe(prompt)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    text_input = \"<s>[INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]\"\n",
        "    tokenized_input = tokenizer(text_input, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input['input_ids']\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=512)\n",
        "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(decoded_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"<s><s> [INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]</s></s>\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "text_input = \"<s>[INST] how do you get to Chad's Gap from the bottom of Alta? [/INST]\"\n",
        "model_input = tokenizer.encode(text_input, return_tensors=\"pt\").to('cuda')\n",
        "generated_ids = model.generate(model_input, max_new_tokens=256, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
